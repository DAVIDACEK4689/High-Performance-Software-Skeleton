#ifndef dcnnsol_hpp_
#define dcnnsol_hpp_

#include "dcnncombined.hpp"

namespace dcnnsol {

	using dcnnasgn::kernel_height_selector;
	using dcnnasgn::kernel_width_selector;
	using dcnnasgn::channel_selector;
	using dcnnasgn::batch_tag;
	using dcnnasgn::height_selector;
	using dcnnasgn::width_selector;
	using dcnnasgn::deep_weight_policy;
	using dcnnasgn::batch_range;
	using dcnnasgn::first_normalize_layer_base;
	using dcnnasgn::deep_cnn_layer_base;
	using dcnnasgn::deep_normalize_layer_base;
	using dcnnasgn::deep_linear_layer_base;
	using dcnnasgn::deep_relu_layer_base;
	using dcnnasgn::deep_maxpool_layer_base;
	using dcnnasgn::aggregate_cnn_layer_base;
	using dcnnasgn::final_linear_layer_base;
	using dcnnasgn::loss_layer_base;
	using dcnnasgn::gold_data;
	using dcnnasgn::deep_data_policy;
	using dcnnasgn::linear_data_policy;
	using dcnnasgn::deep_bias_policy;
	using dcnnasgn::learning_data;

	using tagged::tag_list;
	using tagged::co;
	using tagged::tensor_class;

	/// @addtogroup physlayout 
	/// 
	/// Using the \ref permutation_policy class, 
	/// the physical layout of almost all data can be altered
	/// by permuting the indexes in the respective instances
	/// of \ref tagged::tensor_class
	/// @{

	/// <summary>
	/// @brief Policy class defining the layout of all major data 
	/// 
	/// Each type element is a \ref tagged::tag_list describing the order of indexes in the physical layout of a tensor, 
	/// from the outermost to the innermost
	/// </summary>
	struct permutation_policy {
		/// <summary>
		/// The index order for Convolution and Aggregate layer weights 
		/// (\ref deep_weights::weights, \ref backward_deep_weights::bweights)
		/// </summary>
		using weights = tagged::tag_list< kernel_height_selector, kernel_width_selector, tagged::co<channel_selector>, channel_selector>;
		/// <summary>
		/// The index order for input-dependent learning data on weights (\ref backward_deep_internal::bweightbs)
		/// </summary>
		using weights_batch = tagged::tag_list< batch_tag, kernel_height_selector, kernel_width_selector, tagged::co<channel_selector>, channel_selector>;
		/// <summary>
		/// The index order for forward activation signals (\ref deep_data::values) 
		/// and the corresponding backward-propagates derivatives (\ref backward_deep_data::bvalues) 
		/// between all layers except the first and the last
		/// </summary>
		using data = tagged::tag_list< batch_tag, height_selector, width_selector, channel_selector>;
		/// <summary>
		/// The index order for activation signals (\ref linear_data::values, \ref backward_linear_data::bvalues) generated by the Aggregate layer
		/// </summary>
		using linear_data = tagged::tag_list< batch_tag, channel_selector>;
	};
	/// @}

	/// @addtogroup data 
	///
	/// These classes form the interfaces between layers as well as between forward/backward/learn functions of each layer
	/// 
	/// Each of the classes contains one or more instances of \ref tagged::tensor_class
	/// 
	/// Additional data members may be inserted into these classes if required for optimization
	/// 
	/// Note: The additional data members will typically be redundant versions of the data already contained,
	/// e.g. their transposed versions
	/// 
	/// Temporary data shall be inserted as local variables into the respective functions
	/// @{
	
	/// <summary>
	/// @brief Forward-propagated data between layers
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="SP">The size policy (image size, number of channels)</typeparam>
	template< typename SP>
	class deep_data<SP, permutation_policy> {
	public:
		using policy = deep_data_policy< SP, permutation_policy>;
		deep_data(const batch_range& nr)
			: values(policy::hr& policy::wr& policy::cr& nr)
		{}
		/// <summary>
		/// Activation signals for all images in a minibatch
		/// </summary>
		typename policy::values_t values;
	};

	/// <summary>
	/// @brief Backward-propagated data between layers
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="SP">The size policy (image size, number of channels)</typeparam>
	template< typename SP>
	class backward_deep_data<SP, permutation_policy> {
	public:
		using policy = deep_data_policy< SP, permutation_policy>;
		backward_deep_data(const batch_range& nr)
			: bvalues(~policy::hr& ~policy::wr& ~policy::cr& ~nr)
		{}
		/// <summary>
		/// Partial derivatives of total Loss wrt. activation signals in a minibatch
		/// </summary>
		typename policy::co_values_t bvalues;
	};

	/// <summary>
	/// @brief Output of the final Aggregate layer
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// 
	/// Each element corresponds to a classification category
	/// (i.e. a digit in the MNIST dataset)
	/// </summary>
	/// <typeparam name="SP">The size policy (number of categories)</typeparam>
	template< typename CSP>
	class linear_data< CSP, permutation_policy> {
	public:
		using policy = linear_data_policy< CSP, permutation_policy>;
		linear_data(const batch_range& nr) : values(policy::cr& nr) {}
		/// <summary>
		/// Activation signals for all images in a minibatch
		/// </summary>
		policy::values_t values;
	};

	/// <summary>
	/// @brief Backward-propagated derivatives of the total Loss wrt. the output of the final Aggregate layer
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// 
	/// Each element corresponds to a classification category
	/// (i.e. a digit in the MNIST dataset)
	/// </summary>
	/// <typeparam name="SP">The size policy (number of categories)</typeparam>
	template< typename CSP>
	class backward_linear_data< CSP, permutation_policy> {
	public:
		using policy = linear_data_policy< CSP, permutation_policy>;
		backward_linear_data(const batch_range& nr) : bvalues(~policy::cr& ~nr) {}
		/// <summary>
		/// Partial derivatives of total Loss wrt. activation signals in a minibatch
		/// </summary>
		policy::co_values_t bvalues;
	};

	/// <summary>
	/// @brief Convolution or Aggregate layer model
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="KSP">Kernel size policy</typeparam>
	/// <typeparam name="CSPI">Input channel size policy</typeparam>
	/// <typeparam name="CSPO">Output channel size policy</typeparam>
	template< typename KSP, typename CSPI, typename CSPO>
	class deep_weights< KSP, CSPI, CSPO, permutation_policy> {
	public:
		using policy = deep_weight_policy< KSP, CSPI, CSPO, permutation_policy>;
		/// <summary>
		/// Convolution or Aggregate layer weights
		/// </summary>
		typename policy::weights_t weights;

		deep_weights() 
			: weights(policy::khr& policy::kwr & ~policy::cir& policy::cor) 
		{}
	};

	/// <summary>
	/// @brief Loss gradient over weights for a Convolution or Aggregate layer model
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="KSP">Kernel size policy</typeparam>
	/// <typeparam name="CSPI">Input channel size policy</typeparam>
	/// <typeparam name="CSPO">Output channel size policy</typeparam>
	template< typename KSP, typename CSPI, typename CSPO>
	class backward_deep_weights< KSP, CSPI, CSPO, permutation_policy> {
	public:
		using policy = deep_weight_policy< KSP, CSPI, CSPO, permutation_policy>;
		/// <summary>
		/// The partial derivatives of total Loss wrt. the layer weights
		/// </summary>
		typename policy::co_weights_t bweights;
		/// <summary>
		/// The learning data for sliding normalization 
		/// </summary>
		typename policy::ld_t ld;

		backward_deep_weights() 
			: bweights(~policy::khr& ~policy::kwr & policy::cir& ~policy::cor), 
			ld(policy::cor) 
		{}
	};

	/// <summary>
	/// @brief Internal data for a Convolution or Aggregate layer
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// 
	/// Used to control the normalization of the loss gradient
	/// </summary>
	/// <typeparam name="KSP">Kernel size policy</typeparam>
	/// <typeparam name="CSPI">Input channel size policy</typeparam>
	/// <typeparam name="CSPO">Output channel size policy</typeparam>
	template< typename KSP, typename CSPI, typename CSPO>
	class backward_deep_internal< KSP, CSPI, CSPO, permutation_policy> {
	public:
		using policy = deep_weight_policy< KSP, CSPI, CSPO, permutation_policy>;
		/// <summary>
		/// Partial derivatives of total loss wrt. weights
		/// related to each input image in a minibatch
		/// </summary>
		typename policy::co_weightbs_t bweightbs;

		backward_deep_internal(const batch_range& br)
			: bweightbs(~policy::khr& ~policy::kwr & policy::cir& ~policy::cor& ~br)
		{}
	};

	/// <summary>
	/// @brief Linear layer model
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="CSP">Channel size policy</typeparam>
	template< typename CSP>
	class deep_bias< CSP, permutation_policy> {
	public:
		using policy = deep_bias_policy< CSP, permutation_policy>;
		deep_bias() : betas(policy::cr) {}
		policy::bias_t betas;
	};

	/// <summary>
	/// Loss gradient for Linear layer model
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="CSP">Channel size policy</typeparam>
	template< typename CSP>
	class backward_deep_bias< CSP, permutation_policy> {
	public:
		using policy = deep_bias_policy< CSP, permutation_policy>;
		backward_deep_bias() : bbetas(~policy::cr), ld(policy::cr) {}
		policy::co_bias_t bbetas;
		learning_data<typename policy::channel_tag> ld;
	};
	/// @}

	/// @addtogroup layers
	/// Each structure is an uninstantiated struct containing 
	/// the static member functions implementing
	/// the initialization, forward/backward propagation, and learning
	/// of the layer
	/// 
	/// Each layer struct inherits from a @c _base class
	/// that defines layer-specific types, constants, and functions
	/// @{

	/// <summary>
	/// @brief The first Normalizing layer
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="SP">Image size policy</typeparam>
	template< typename SP>
	struct first_normalize_layer< SP, permutation_policy>
		: first_normalize_layer_base< SP, permutation_policy>
	{
	private:
		using base_ = first_normalize_layer_base< SP, permutation_policy>;

		using typename base_::input_data;
		using typename base_::stat_data;
		using typename base_::output_data;

		using base_::hr;	
		using base_::wr;	
		//using base_::cor;	
		using base_::coi;	

		using base_::forward_check;	
	public:
		/// <summary>
		/// @brief The forward-propagation function of the first Normalizing layer
		/// </summary>
		/// <param name="ind">Input images of a minibatch</param>
		/// <param name="cd">Normalization statistics</param>
		/// <param name="outd">Normalized images</param>
		static void forward(const input_data& ind, stat_data& cd, output_data& outd)
		{
			auto br = forward_check(ind, cd, outd);

			auto&& inv = ind.images;
			auto&& sumx = cd.sumx;
			auto&& sumx2 = cd.sumx2;
			auto&& E = cd.E;
			auto&& var = cd.var;
			auto&& scale = cd.scale;
			auto&& outv = outd.values;

			sumx = 0;
			sumx2 = 0;

			for (auto h : hr)
			{
				for (auto w : wr)
				{
					for (auto b : br)
					{
						uint64_t value = inv[b & h & w];
						sumx += value;
						sumx2 += value * value;
					}
				}
			}

			std::uint64_t n = hr.size() * wr.size() * br.size();
			E = (float)((double)sumx / n);
			var = (float)(((double)sumx2 - (double)sumx * E) / n);
			scale = 1.0f / std::sqrt(var);

			for (auto h : hr)
			{
				for (auto w : wr)
				{
					for (auto b : br)
					{
						outv[h & w & coi & b] = scale * (inv[b & h & w] - E);
					}
				}
			}
		}
	};

	/// <summary>
	/// @brief A convolutional layer
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="SPI">Input size policy</typeparam>
	/// <typeparam name="SPO">Output size policy</typeparam>
	/// <typeparam name="KSP">Kernel size policy</typeparam>
	template< typename SPI, typename SPO, typename KSP>
	struct deep_cnn_layer<SPI, SPO, KSP, permutation_policy>
		: deep_cnn_layer_base< SPI, SPO, KSP, permutation_policy>
	{
	private:
		using base_ = deep_cnn_layer_base< SPI, SPO, KSP, permutation_policy>;

		using typename base_::input_data;
		using typename base_::weights;
		using typename base_::output_data;
		using typename base_::backward_input_data;
		using typename base_::backward_weights;
		using typename base_::backward_internal;
		using typename base_::backward_output_data;

		using base_::hokrf;
		using base_::hokimf;
		using base_::wokrf;
		using base_::wokimf;

		using base_::hikrf;
		using base_::hikomf;
		using base_::wikrf;
		using base_::wikomf;

		using base_::hkorf;
		using base_::hkoimf;
		using base_::wkorf;
		using base_::wkoimf;

		using base_::hir;
		using base_::wir;
		using base_::cir;
		using base_::khr;
		using base_::kwr;
		using base_::hor;
		using base_::wor;
		using base_::cor;

		using base_::fanin;

		using base_::forward_check;
		using base_::backward_check;
		using base_::learn_check;

		using typename base_::adjustment;
	public:
		/// <summary>
		/// @brief Initialize the model weights randomly
		/// </summary>
		/// <param name="wtd">Model data</param>
		/// <param name="engine">Random engine</param>
		/// <param name="expected_multiplier">Statistic parameter</param>
		static void init_forward(weights& wtd, std::mt19937_64& engine, float expected_multiplier)
		{
			float k = expected_multiplier * std::sqrt(3.0f / fanin);
			std::uniform_real_distribution<float> distro(-k, k);

			auto&& weights = wtd.weights;

			for (auto y : weights.range())
			{
				weights[y] = distro(engine);
			}
		}

		/// <summary>
		/// @brief Initialize the learning data
		/// </summary>
		/// <param name="bwtd">Learning data</param>
		/// <param name="summary_backprop_variance">Initialization parameter</param>
		static void init_backward(backward_weights& bwtd, float summary_backprop_variance)
		{
			adjustment::init(bwtd.ld, summary_backprop_variance);
		}

		/// <summary>
		/// The forward-propagation function of a convolutional layer
		/// </summary>
		/// <param name="ind">Input activation</param>
		/// <param name="wtd">Model weights</param>
		/// <param name="outd">Ourput activation</param>
		static void forward(const input_data& ind, const weights& wtd, output_data& outd)
		{
			auto br = forward_check(ind, wtd, outd);

			auto&& inv = ind.values;
			auto&& wtv = wtd.weights;
			auto&& outv = outd.values;

			for (auto x : outv.range())
			{
				outv[x] = 0.0f;
			}

			for (auto b : br)
			{
				auto&& outvb = outv[b];
				auto&& invb = inv[b];

				for (auto kh : khr)
				{
					auto hor2 = hkorf(kh);
					auto hoid = hkoimf(kh);

					for (auto kw : kwr)
					{
						auto wor2 = wkorf(kw);
						auto woid = wkoimf(kw);

						auto&& wtvk = wtv[kh & kw];

						for (auto ho : hor2)
						{
							auto hi = hoid(ho);

							auto&& outvbh = outvb[ho];
							auto&& invbh = invb[hi];

							for (auto wo : wor2)
							{
								auto wi = woid(wo);

								auto&& outvbhw = outvbh[wo];
								auto&& invbhw = invbh[wi];

								for (auto ci : cir)
								{
									auto&& wtwki = wtvk[~ci];

									float val_inv = invbhw[ci];
									for (auto co : cor)
									{
										outvbhw[co] += wtwki[co] * val_inv;
									}
								}
							}
						}
					}
				}
			}
		}

		/// <summary>
		/// The backward-propagation function of a convolutional layer
		/// </summary>
		/// <param name="ind">Input activation</param>
		/// <param name="wtd">Model weights</param>
		/// <param name="boutd">Loss derivatives wrt. output activation</param>
		/// <param name="bwtd">Computed loss gradient over weights</param>
		/// <param name="bid">Computed detailed loss gradient over weights and samples</param>
		/// <param name="bind">Computed loss derivatives wrt. input activation</param>
		static void backward(const input_data& ind, const weights& wtd, const backward_output_data& boutd, backward_weights& bwtd, backward_internal& bid, backward_input_data& bind)
		{
			auto br = backward_check(ind, wtd, boutd, bwtd, bid, bind);

			auto&& inv = ind.values;
			auto&& wtv = wtd.weights;
			auto&& boutv = boutd.bvalues;
			auto&& bwtv = bwtd.bweights;
			auto&& bwtbv = bid.bweightbs;
			auto&& binv = bind.bvalues;

			for (auto x : bwtv.range())
			{
				bwtv[x] = 0.0f;
			}

			for (auto x : bwtbv.range())
			{
				bwtbv[x] = 0.0f;
			}

			for (auto x : binv.range())
			{
				binv[x] = 0.0f;
			}

			for (auto b : br)
			{
				auto&& boutvb = boutv[~b];
				auto&& bwtbvb = bwtbv[~b];
				auto&& invb = inv[b];
				auto&& binvb = binv[~b];

				for (auto kh : khr)
				{
					auto hor2 = hkorf(kh);
					auto hoid = hkoimf(kh);

					for (auto kw : kwr)
					{
						auto wor2 = wkorf(kw);
						auto woid = wkoimf(kw);

						auto&& wtvk = wtv[kh & kw];
						auto&& bwtvk = bwtv[~kh & ~kw];
						auto&& bwtbvbk = bwtbvb[~kh & ~kw];

						for (auto ho : hor2)
						{
							auto hi = hoid(ho);

							auto&& boutvbh = boutvb[~ho];
							auto&& invbh = invb[hi];
							auto&& binvbh = binvb[~hi];

							for (auto wo : wor2)
							{
								auto wi = woid(wo);

								auto&& boutvbhw = boutvbh[~wo];
								auto&& invbhw = invbh[wi];
								auto&& binvbhw = binvbh[~wi];

								for (auto ci : cir)
								{
									auto&& wtvki = wtvk[~ci];
									auto&& bwtvki = bwtvk[ci];
									auto&& bwtbvbki = bwtbvbk[ci];

									float val_inv = invbhw[ci];
									float sum_binv = 0.0f;
									for (auto co : cor)
									{
										auto val_boutv = boutvbhw[~co];
										sum_binv += wtvki[co] * val_boutv;
										auto value = val_inv * val_boutv;
										bwtbvbki[~co] += value;
										bwtvki[~co] += value;
									}
									binvbhw[~ci] += sum_binv;
								}
							}
						}
					}
				}
			}

			adjustment::zeros(bwtd.ld);

			auto&& sumx2v = bwtd.ld.sumx2v;

			for (auto b : br)
			{
				auto&& bvb = bwtbv[~b];
				for (auto kh : khr)
				{
					auto&& bvbh = bvb[~kh];
					for (auto kw : kwr)
					{
						auto&& bvbhw = bvbh[~kw];
						for (auto ci : cir)
						{
							auto&& bvbhwi = bvbhw[ci];
							for (auto co : cor)
							{
								auto value = bvbhwi[~co];
								sumx2v[co] += value * value;
							}
						}
					}
				}
			}

			adjustment::setup(bwtd.ld);
		}

		/// <summary>
		/// The learning function of a convolutional layer
		/// </summary>
		/// <param name="bwtd">Loss gradient over weights</param>
		/// <param name="wtd">Model to be adjusted</param>
		/// <param name="rate">Adjustment rate</param>
		static void learn(const backward_weights& bwtd, weights& wtd, float rate)
		{
			learn_check(bwtd, wtd);

			auto&& wtv = wtd.weights;
			auto&& bwtv = bwtd.bweights;
			auto&& scalev = bwtd.ld.scale;

			for (auto kh : khr)
			{
				for (auto kw : kwr)
				{
					for (auto ci : cir)
					{
						for (auto co : cor)
						{
							auto scale = scalev[co];
							wtv[kh & kw & ~ci & co] -= rate * scale * bwtv[~kh & ~kw & ci & ~co];
						}
					}
				}
			}
		}
	};

	/// <summary>
	/// @brief A normalizing layer
	/// 
	/// Specialized for the selected \ref permutation_policy
	/// </summary>
	/// <typeparam name="SP">Size policy</typeparam>
	template< typename SP>
	struct deep_normalize_layer< SP, permutation_policy>
		: deep_normalize_layer_base<SP, permutation_policy>
	{
	private:
		using self_ = deep_normalize_layer_base<SP, permutation_policy>;

		using typename self_::input_data;
		using typename self_::stat_data;
		using typename self_::output_data;

		using typename self_::backward_input_data;
		using typename self_::backward_output_data;

		using typename self_::channel_tag;

		using self_::hr;
		using self_::wr;
		using self_::cr;

		using self_::forward_check;
		using self_::backward_check;
	public:
		/// <summary>
		/// The forward-propagation function of a normalizing layer
 		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="cd">Statistical properties of the input activations</param>
		/// <param name="outd">Output activations</param>
		static void forward(const input_data& ind, stat_data& cd, output_data& outd)
		{
			auto br = forward_check(ind, cd, outd);

			auto&& inv = ind.values;
			auto&& sumxv = cd.sumxs;
			auto&& sumx2v = cd.sumx2s;
			auto&& Ev = cd.Es;
			auto&& varv = cd.vars;
			auto&& scalev = cd.scales;
			auto&& outv = outd.values;

			std::uint64_t n = hr.size() * wr.size() * br.size();

			for (auto c : cr)
			{
				sumxv[c] = 0.0f;
				sumx2v[c] = 0.0f;
			}

			for (auto b : br)
			{
				auto&& invb = inv[b];
				for (auto h : hr)
				{
					auto&& invbh = invb[h];
					for (auto w : wr)
					{
						auto&& invbhw = invbh[w];
						for (auto c : cr)
						{
							auto value = invbhw[c];
							sumxv[c] += value;
							sumx2v[c] += value * value;
						}
					}
				}
			}

			for (auto c : cr)
			{
				float sumx = sumxv[c];
				float sumx2 = sumx2v[c];

				float E = sumx / n;
				float var = (sumx2 - sumx * E) / n;
				float scale = 1.0f / std::sqrt(var);

				Ev[c] = E;
				varv[c] = var;
				scalev[c] = scale;
			}

			for (auto b : br)
			{
				auto&& invb = inv[b];
				auto&& outvb = outv[b];
				for (auto h : hr)
				{
					auto&& invbh = invb[h];
					auto&& outvbh = outvb[h];
					for (auto w : wr)
					{
						auto&& invbhw = invbh[w];
						auto&& outvbhw = outvbh[w];
						for (auto c : cr)
						{
							outvbhw[c] = scalev[c] * (invbhw[c] - Ev[c]);
						}
					}
				}
			}
		}

		/// <summary>
		/// The backward-propagation function of a normalizing layer
		/// </summary>
		/// <param name="cd">Statistical properties</param>
		/// <param name="outd">Output activations</param>
		/// <param name="boutd">Loss derivatives wrt. output activations</param>
		/// <param name="bind">Computed loss derivatives wrt. input activations</param>
		static void backward(const stat_data& cd, const output_data& outd, const backward_output_data& boutd, backward_input_data& bind)
		{
			auto br = backward_check(cd, outd, boutd, bind);

			auto&& outv = outd.values;
			auto&& scalev = cd.scales;
			auto&& boutv = boutd.bvalues;
			auto&& binv = bind.bvalues;

			tagged::tensor_class< float, channel_tag> sbov(cr);
			tagged::tensor_class< float, channel_tag> sobov(cr);

			std::uint64_t n = hr.size() * wr.size() * br.size();
			auto invn = 1.0f / n;

			for (auto c : cr)
			{
				sbov[c] = 0.0f;
				sobov[c] = 0.0f;
			}

			for (auto b : br)
			{
				auto&& boutvb = boutv[~b];
				auto&& outvb = outv[b];
				for (auto h : hr)
				{
					auto&& boutvbh = boutvb[~h];
					auto&& outvbh = outvb[h];
					for (auto w : wr)
					{
						auto&& boutvbhw = boutvbh[~w];
						auto&& outvbhw = outvbh[w];
						for (auto c : cr)
						{
							auto bout = boutvbhw[~c];
							sbov[c] += bout;
							sobov[c] += outvbhw[c] * bout;
						}
					}
				}
			}

			for (auto c : cr)
			{
				sbov[c] = invn * sbov[c];
				sobov[c] = invn * sobov[c];
			}

			for (auto b : br)
			{
				auto&& binvb = binv[~b];
				auto&& boutvb = boutv[~b];
				auto&& outvb = outv[b];
				for (auto h : hr)
				{
					auto&& binvbh = binvb[~h];
					auto&& boutvbh = boutvb[~h];
					auto&& outvbh = outvb[h];
					for (auto w : wr)
					{
						auto&& binvbhw = binvbh[~w];
						auto&& boutvbhw = boutvbh[~w];
						auto&& outvbhw = outvbh[w];
						for (auto c : cr)
						{
							binvbhw[~c] = scalev[c] * (boutvbhw[~c] - sbov[c] - sobov[c] * outvbhw[c]);
						}
					}
				}
			}
		}
	};

	/// <summary>
	/// Linear layer 
	/// 
	/// Performs addition of a learned channel-specific bias
	/// </summary>
	/// <typeparam name="SP">Size policy</typeparam>
	template< typename SP>
	struct deep_linear_layer< SP, permutation_policy>
		: deep_linear_layer_base< SP, permutation_policy>
	{
	private:
		using base_ = deep_linear_layer_base< SP, permutation_policy>;

		using typename base_::input_data;
		using typename base_::bias_data;
		using typename base_::output_data;

		using typename base_::backward_input_data;
		using typename base_::backward_bias_data;
		using typename base_::backward_output_data;

		using base_::hr;
		using base_::wr;
		using base_::cr;

		using base_::forward_check;
		using base_::backward_check;
		using base_::learn_check;

		using typename base_::adjustment;
	public:
		/// <summary>
		/// Setup all biases to the given value
		/// </summary>
		/// <param name="cd">Layer model (biases)</param>
		/// <param name="initial_bias">Initial value</param>
		static void init_forward(bias_data& cd, float initial_bias)
		{
			for (auto x : cr)
			{
				cd.betas[x] = initial_bias;
			}
		}

		/// <summary>
		/// Initialize learning properties
		/// </summary>
		/// <param name="bcd">Learning data</param>
		/// <param name="summary_backprop_variance">Required statistical variance</param>
		static void init_backward(backward_bias_data& bcd, float summary_backprop_variance)
		{
			adjustment::init(bcd.ld, summary_backprop_variance);
		}

		/// <summary>
		/// The forward-propagation function of a linear layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="cd">Layer model (biases)</param>
		/// <param name="outd">Output activations</param>
		static void forward(const input_data& ind, const bias_data& cd, output_data& outd)
		{
			auto br = forward_check(ind, cd, outd);

			auto&& inv = ind.values;
			auto&& betav = cd.betas;
			auto&& outv = outd.values;

			for (auto b : br)
			{
				auto&& invb = inv[b];
				auto&& outvb = outv[b];
				for (auto h : hr)
				{
					auto&& invbh = invb[h];
					auto&& outvbh = outvb[h];
					for (auto w : wr)
					{
						auto&& invbhw = invbh[w];
						auto&& outvbhw = outvbh[w];
						for (auto c : cr)
						{
							outvbhw[c] = invbhw[c] + betav[c];
						}
					}
				}
			}
		}

		/// <summary>
		/// The backward-propagation function of a linear layer
		/// </summary>
		/// <param name="boutd">Loss derivatives wrt. output activations</param>
		/// <param name="bcd">Learning data</param>
		/// <param name="bind">Loss derivatives wrt. input activations</param>
		static void backward(const backward_output_data& boutd, backward_bias_data& bcd, backward_input_data& bind)
		{
			auto br = backward_check(boutd, bcd, bind);

			auto&& boutv = boutd.bvalues;
			auto&& bbetav = bcd.bbetas;
			auto&& binv = bind.bvalues;

			for (auto c : cr)
			{
				bbetav[~c] = 0.0f;
			}

			for (auto b : br)
			{
				auto&& binvb = binv[~b];
				auto&& boutvb = boutv[~b];
				for (auto h : hr)
				{
					auto&& binvbh = binvb[~h];
					auto&& boutvbh = boutvb[~h];
					for (auto w : wr)
					{
						auto&& binvbhw = binvbh[~w];
						auto&& boutvbhw = boutvbh[~w];
						for (auto c : cr)
						{
							auto bout = boutvbhw[~c];
							binvbhw[~c] = bout;
							bbetav[~c] += bout;
						}
					}
				}
			}

			adjustment::zeros(bcd.ld);

			auto&& sumx2v = bcd.ld.sumx2v;

			for (auto b : br)
			{
				auto&& bvb = binv[~b];
				for (auto h : hr)
				{
					auto&& bvbh = bvb[~h];
					for (auto w : wr)
					{
						auto&& bvbhw = bvbh[~w];
						for (auto c : cr)
						{
							auto value = bvbhw[~c];
							sumx2v[c] += value * value;
						}
					}
				}
			}

			adjustment::setup(bcd.ld);
		}

		/// <summary>
		/// The learning function of a linear layer
		/// </summary>
		/// <param name="bcd">Learning data</param>
		/// <param name="cd">Layer model (biases)</param>
		/// <param name="rate">Learning rate</param>
		static void learn(const backward_bias_data& bcd, bias_data& cd, float rate)
		{
			learn_check(bcd, cd);

			auto&& bbetav = bcd.bbetas;
			auto&& scalev = bcd.ld.scale;
			auto&& betav = cd.betas;

			for (auto c : cr)
			{
				betav[c] -= rate * scalev[c] * bbetav[~c];
			}
		}
	};

	/// <summary>
	/// A ReLU layer
	/// </summary>
	/// <typeparam name="SP">Size policy</typeparam>
	template< typename SP>
	struct deep_relu_layer< SP, permutation_policy>
		: deep_relu_layer_base< SP, permutation_policy>
	{
	private:
		using base_ = deep_relu_layer_base< SP, permutation_policy>;

		using typename base_::input_data;
		using typename base_::output_data;

		using typename base_::backward_output_data;
		using typename base_::backward_input_data;

		using base_::hr;
		using base_::wr;
		using base_::cr;

		using base_::forward_check;
		using base_::backward_check;
	public:
		/// <summary>
		/// The forward-propagation function of a ReLU layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="outd">Output activations</param>
		static void forward(const input_data& ind, output_data& outd)
		{
			auto br = forward_check(ind, outd);

			auto&& inv = ind.values;
			auto&& outv = outd.values;

			for (auto b : br)
			{
				auto&& invb = inv[b];
				auto&& outvb = outv[b];
				for (auto h : hr)
				{
					auto&& invbh = invb[h];
					auto&& outvbh = outvb[h];
					for (auto w : wr)
					{
						auto&& invbhw = invbh[w];
						auto&& outvbhw = outvbh[w];
						for (auto c : cr)
						{
							outvbhw[c] = std::max(0.0f, invbhw[c]);
						}
					}
				}
			}
		}

		/// <summary>
		/// The backward-propagation function of a ReLU layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="boutd">Loss derivatives wrt. output activations</param>
		/// <param name="bind">Loss derivatives wrt. input activations</param>
		static void backward(const input_data& ind, const backward_output_data& boutd, backward_input_data& bind)
		{
			auto br = backward_check(ind, boutd, bind);

			auto&& inv = ind.values;
			auto&& boutv = boutd.bvalues;
			auto&& binv = bind.bvalues;

			for (auto b : br)
			{
				auto&& invb = inv[b];
				auto&& binvb = binv[~b];
				auto&& boutvb = boutv[~b];
				for (auto h : hr)
				{
					auto&& invbh = invb[h];
					auto&& binvbh = binvb[~h];
					auto&& boutvbh = boutvb[~h];
					for (auto w : wr)
					{
						auto&& invbhw = invbh[w];
						auto&& binvbhw = binvbh[~w];
						auto&& boutvbhw = boutvbh[~w];
						for (auto c : cr)
						{
							binvbhw[~c] = invbhw[c] >= 0.0f ? boutvbhw[~c] : 0.0f;
						}
					}
				}
			}
		}
	};

	/// <summary>
	/// A MaxPool layer
	/// </summary>
	/// <typeparam name="SPI">Input size policy</typeparam>
	/// <typeparam name="SPO">Output size policy</typeparam>
	template< typename SPI, typename SPO>
	struct deep_maxpool_layer< SPI, SPO, permutation_policy>
		: deep_maxpool_layer_base<SPI, SPO, permutation_policy>
	{
	private:
		using base_ = deep_maxpool_layer_base<SPI, SPO, permutation_policy>;

		using typename base_::input_data;
		using typename base_::output_data;

		using typename base_::backward_output_data;
		using typename base_::backward_input_data;

		using typename base_::height_out_tag;
		using typename base_::width_out_tag;
		using typename base_::channel_tag;

		using base_::hir;
		using base_::wir;
		using base_::cr;

		using base_::khr;
		using base_::kwr;

		using base_::hor;
		using base_::wor;

		using base_::hkoimf;
		using base_::wkoimf;

		using base_::forward_check;
		using base_::backward_check;
	public:
		/// <summary>
		/// The forward-propagation function of a MaxPool layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="outd">Output activations</param>
		static void forward(const input_data& ind, output_data& outd)
		{
			auto br = forward_check(ind, outd);

			auto&& inv = ind.values;
			auto&& outv = outd.values;

			for (auto b : br)
			{
				auto&& outvb = outv[b];
				for (auto ho : hor)
				{
					auto&& outvbh = outvb[ho];
					for (auto wo : wor)
					{
						auto&& outvbhw = outvbh[wo];
						for (auto c : cr)
						{
							outvbhw[c] = -std::numeric_limits<float>::infinity();
						}
					}
				}
			}

			for (auto b : br)
			{
				auto&& invb = inv[b];
				auto&& outvb = outv[b];
				for (auto kh : khr)
				{
					auto hoid = hkoimf(kh);
					for (auto kw : kwr)
					{
						auto woid = wkoimf(kw);
						for (auto ho : hor)
						{
							auto&& outvbh = outvb[ho];
							auto hi = hoid(ho);
							auto&& invbh = invb[hi];
							for (auto wo : wor)
							{
								auto&& outvbhw = outvbh[wo];
								auto wi = woid(wo);
								auto&& invbhw = invbh[wi];
								for (auto c : cr)
								{
									outvbhw[c] = std::max(outvbhw[c], invbhw[c]);
								}
							}
						}
					}
				}
			}
		}

		/// <summary>
		/// The backward-propagation function of a MaxPool layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="outd">Output activations</param>
		/// <param name="boutd">Loss derivatives wrt. output activations</param>
		/// <param name="bind">Loss derivatives wrt. input activations</param>
		static void backward(const input_data& ind, const output_data& outd, const backward_output_data& boutd, backward_input_data& bind)
		{
			auto br = backward_check(ind, outd, boutd, bind);

			auto&& inv = ind.values;
			auto&& outv = outd.values;
			auto&& boutv = boutd.bvalues;
			auto&& binv = bind.bvalues;

			tagged::tensor_class< float, batch_tag, height_out_tag, width_out_tag, channel_tag> wv(br & hor & wor & cr);

			for (auto b : br)
			{
				auto&& wvb = wv[b];
				for (auto ho : hor)
				{
					auto&& wvbh = wvb[ho];
					for (auto wo : wor)
					{
						auto&& wvbhw = wvbh[wo];
						for (auto c : cr)
						{
							wvbhw[c] = 0.0f;
						}
					}
				}
			}

			for (auto b : br)
			{
				auto&& wvb = wv[b];
				auto&& invb = inv[b];
				auto&& outvb = outv[b];
				for (auto kh : khr)
				{
					auto hoid = hkoimf(kh);
					for (auto kw : kwr)
					{
						auto woid = wkoimf(kw);
						for (auto ho : hor)
						{
							auto&& wvbh = wvb[ho];
							auto&& outvbh = outvb[ho];
							auto hi = hoid(ho);
							auto&& invbh = invb[hi];
							for (auto wo : wor)
							{
								auto&& wvbhw = wvbh[wo];
								auto&& outvbhw = outvbh[wo];
								auto wi = woid(wo);
								auto&& invbhw = invbh[wi];
								for (auto c : cr)
								{
									wvbhw[c] += invbhw[c] == outvbhw[c] ? 1.0f : 0.0f;
								}
							}
						}
					}
				}
			}

			for (auto b : br)
			{
				auto&& wvb = wv[b];
				for (auto ho : hor)
				{
					auto&& wvbh = wvb[ho];
					for (auto wo : wor)
					{
						auto&& wvbhw = wvbh[wo];
						for (auto c : cr)
						{
							wvbhw[c] = 1.0f / wvbhw[c];
						}
					}
				}
			}

			for (auto b : br)
			{
				auto&& wvb = wv[b];
				auto&& invb = inv[b];
				auto&& binvb = binv[~b];
				auto&& outvb = outv[b];
				auto&& boutvb = boutv[~b];
				for (auto kh : khr)
				{
					auto hoid = hkoimf(kh);
					for (auto kw : kwr)
					{
						auto woid = wkoimf(kw);
						for (auto ho : hor)
						{
							auto&& wvbh = wvb[ho];
							auto&& outvbh = outvb[ho];
							auto&& boutvbh = boutvb[~ho];
							auto hi = hoid(ho);
							auto&& invbh = invb[hi];
							auto&& binvbh = binvb[~hi];
							for (auto wo : wor)
							{
								auto&& wvbhw = wvbh[wo];
								auto&& outvbhw = outvbh[wo];
								auto&& boutvbhw = boutvbh[~wo];
								auto wi = woid(wo);
								auto&& invbhw = invbh[wi];
								auto&& binvbhw = binvbh[~wi];
								for (auto c : cr)
								{
									binvbhw[~c] = invbhw[c] == outvbhw[c] ? wvbhw[c] : 0.0f;
									binvbhw[~c] *= boutvbhw[~c];
								}
							}
						}
					}
				}
			}
		}
	};

	/// <summary>
	/// An aggregate layer
	/// 
	/// Produces a linear data, i.e. a single value for each output channel,
	/// by weighed aggregation of an input image across all points and input channels 
	/// </summary>
	/// <typeparam name="SPI">Input size policy</typeparam>
	/// <typeparam name="CSPO">Output channel policy</typeparam>
	template< typename SPI, typename CSPO>
	struct aggregate_cnn_layer< SPI, CSPO, permutation_policy>
		: aggregate_cnn_layer_base< SPI, CSPO, permutation_policy>
	{
	private:
		using base_ = aggregate_cnn_layer_base< SPI, CSPO, permutation_policy>;

		using typename base_::input_data;
		using typename base_::weights;
		using typename base_::output_data;
		using typename base_::backward_input_data;
		using typename base_::backward_weights;
		using typename base_::backward_internal;
		using typename base_::backward_output_data;

		using base_::hir;
		using base_::wir;
		using base_::cir;
		using base_::cor;

		using base_::hikmf;
		using base_::wikmf;

		using base_::forward_check;
		using base_::backward_check;
		using base_::learn_check;

		using base_::fanin;

		using typename base_::adjustment;
	public:
		/// <summary>
		/// Init the weights randomly
		/// </summary>
		/// <param name="wtd">Layer model (weights)</param>
		/// <param name="engine">Random engine</param>
		/// <param name="expected_multiplier">Expected multiplier</param>
		static void init_forward(weights& wtd, std::mt19937_64& engine, float expected_multiplier)
		{
			float k = expected_multiplier * std::sqrt(3.0f / fanin);
			std::uniform_real_distribution<float> distro(-k, k);

			auto&& weights = wtd.weights;

			for (auto y : weights.range())
			{
				weights[y] = distro(engine);
			}
		}

		/// <summary>
		/// Init the learning data
		/// </summary>
		/// <param name="bwtd">Learning data</param>
		/// <param name="summary_backprop_variance">Statistic parameter</param>
		static void init_backward(backward_weights& bwtd, float summary_backprop_variance)
		{
			adjustment::init(bwtd.ld, summary_backprop_variance);
		}

		/// <summary>
		/// @brief The forward-propagation function of an aggregate layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="wtd">Layer model (weights)</param>
		/// <param name="outd">Output activations</param>
		static void forward(const input_data& ind, const weights& wtd, output_data& outd)
		{
			auto br = forward_check(ind, wtd, outd);

			auto&& inv = ind.values;
			auto&& wtv = wtd.weights;
			auto&& outv = outd.values;

			for (auto co : cor)
			{
				for (auto b : br)
				{
					outv[co & b] = 0.0f;
				}
			}

			for (auto hi : hir)
			{
				auto kh = hikmf(hi);
				for (auto wi : wir)
				{
					auto kw = wikmf(wi);
					for (auto ci : cir)
					{
						for (auto co : cor)
						{
							auto wt = wtv[kh & kw & ~ci & co];
							for (auto b : br)
							{
								outv[co & b] += wt * inv[hi & wi & ci & b];
							}
						}
					}
				}
			}
		}

		/// <summary>
		/// @brief The forward-propagation function of an aggregate layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="wtd">Layer model (weights)</param>
		/// <param name="boutd">Loss derivatives wrt. output activations</param>
		/// <param name="bwtd">Loss derivatives wrt. weights</param>
		/// <param name="bid">Detailed loss derivatives wrt. weights and inputs</param>
		/// <param name="bind">Loss derivatives wrt. input activations</param>
		static void backward(const input_data& ind, const weights& wtd, const backward_output_data& boutd, backward_weights& bwtd, backward_internal& bid, backward_input_data& bind)
		{
			auto br = backward_check(ind, wtd, boutd, bwtd, bid, bind);

			auto&& inv = ind.values;
			auto&& wtv = wtd.weights;
			auto&& boutv = boutd.bvalues;
			auto&& bwtv = bwtd.bweights;
			auto&& bwtbv = bid.bweightbs;
			auto&& binv = bind.bvalues;

			for (auto x : bwtv.range())
			{
				bwtv[x] = 0.0f;
			}

			for (auto x : bwtbv.range())
			{
				bwtbv[x] = 0.0f;
			}

			for (auto x : binv.range())
			{
				binv[x] = 0.0f;
			}

			for (auto hi : hir)
			{
				auto kh = hikmf(hi);
				for (auto wi : wir)
				{
					auto kw = wikmf(wi);
					for (auto ci : cir)
					{
						for (auto co : cor)
						{
							auto wt = wtv[kh & kw & ~ci & co];
							auto bwt = 0.0f;
							for (auto b : br)
							{
								binv[~hi & ~wi & ~ci & ~b] += wt * boutv[~co & ~b];
								auto value = inv[hi & wi & ci & b] * boutv[~co & ~b];
								bwtbv[~kh & ~kw & ci & ~co & ~b] += value;
								bwt += value;
							}
							bwtv[~kh & ~kw & ci & ~co] += bwt;
						}
					}
				}
			}

			adjustment::zeros(bwtd.ld);

			auto&& sumx2v = bwtd.ld.sumx2v;

			for (auto b : br)
			{
				auto&& bvb = bwtbv[~b];
				for (auto hi : hir)
				{
					auto kh = hikmf(hi);
					auto&& bvbh = bvb[~kh];
					for (auto wi : wir)
					{
						auto kw = wikmf(wi);
						auto&& bvbhw = bvbh[~kw];
						for (auto ci : cir)
						{
							auto&& bvbhwi = bvbhw[ci];
							for (auto co : cor)
							{
								auto value = bvbhwi[~co];
								sumx2v[co] += value * value;
							}
						}
					}
				}
			}

			adjustment::setup(bwtd.ld);
		}

		/// <summary>
		/// @brief The learn function of an aggregate layer
		/// </summary>
		/// <param name="bwtd">Loss derivatives wrt. weights</param>
		/// <param name="wtd">Layer model (weights)</param>
		/// <param name="rate">Learning rate</param>
		static void learn(const backward_weights& bwtd, weights& wtd, float rate)
		{
			learn_check(bwtd, wtd);

			auto&& wtv = wtd.weights;
			auto&& bwtv = bwtd.bweights;
			auto&& scalev = bwtd.ld.scale;

			for (auto hi : hir)
			{
				auto kh = hikmf(hi);
				for (auto wi : wir)
				{
					auto kw = wikmf(wi);
					for (auto ci : cir)
					{
						for (auto co : cor)
						{
							auto scale = scalev[co];
							wtv[kh & kw & ~ci & co] -= rate * scale * bwtv[~kh & ~kw & ci & ~co];
						}
					}
				}
			}
		}
	};

	/// <summary>
	/// Final linear layer
	/// 
	/// Performs addition of a learned channel-specific bias to linear data
	/// </summary>
	/// <typeparam name="CSP">Channel size policy</typeparam>
	template< typename CSP>
	struct final_linear_layer< CSP, permutation_policy>
		: final_linear_layer_base< CSP, permutation_policy>
	{
	private:
		using base_ = final_linear_layer_base< CSP, permutation_policy>;

		using typename base_::input_data;
		using typename base_::bias_data;
		using typename base_::output_data;

		using typename base_::backward_input_data;
		using typename base_::backward_bias_data;
		using typename base_::backward_output_data;

		using base_::cr;

		using base_::forward_check;
		using base_::backward_check;
		using base_::learn_check;

		using typename base_::adjustment;
	public:
		/// <summary>
		/// Initialize the model with the given value
		/// </summary>
		/// <param name="cd">Layer model (biases)</param>
		/// <param name="initial_bias">Initial value</param>
		static void init_forward(bias_data& cd, float initial_bias)
		{
			for (auto x : cr)
			{
				cd.betas[x] = initial_bias;
			}
		}

		/// <summary>
		/// Initialize learning parameters
		/// </summary>
		/// <param name="bcd">Learning data</param>
		/// <param name="summary_backprop_variance">Required statistical properties</param>
		static void init_backward(backward_bias_data& bcd, float summary_backprop_variance)
		{
			adjustment::init(bcd.ld, summary_backprop_variance);
		}

		/// <summary>
		/// The forward-propagation function of a linear layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="cd">Layer model (biases)</param>
		/// <param name="outd">Output activations</param>
		static void forward(const input_data& ind, const bias_data& cd, output_data& outd)
		{
			auto br = forward_check(ind, cd, outd);

			auto&& inv = ind.values;
			auto&& betav = cd.betas;
			auto&& outv = outd.values;

			for (auto c : cr)
			{
				auto beta = betav[c];
				for (auto b : br)
				{
					outv[c & b] = inv[c & b] + beta;
				}
			}
		}

		/// <summary>
		/// The backward-propagation function of a linear layer
		/// </summary>
		/// <param name="boutd">Loss derivatives wrt. output activations</param>
		/// <param name="bcd">Loss derivatives wrt. biases</param>
		/// <param name="bind">Loss derivatives wrt. input activations</param>
		static void backward(const backward_output_data& boutd, backward_bias_data& bcd, backward_input_data& bind)
		{
			auto br = backward_check(boutd, bcd, bind);

			auto&& boutv = boutd.bvalues;
			auto&& bbetav = bcd.bbetas;
			auto&& binv = bind.bvalues;

			for (auto c : cr)
			{
				auto bbeta = 0.0f;
				for (auto b : br)
				{
					auto bout = boutv[~c & ~b];
					binv[~c & ~b] = bout;
					bbeta += bout;
				}
				bbetav[~c] = bbeta;
			}

			adjustment::zeros(bcd.ld);

			auto&& sumx2v = bcd.ld.sumx2v;

			for (auto b : br)
			{
				auto&& bvb = binv[~b];
				for (auto c : cr)
				{
					auto value = bvb[~c];
					sumx2v[c] += value * value;
				}
			}

			adjustment::setup(bcd.ld);
		}

		/// <summary>
		/// The learn function of a linear layer
		/// </summary>
		/// <param name="bcd">Loss derivatives wrt. biases</param>
		/// <param name="cd">Layer model (biases)</param>
		/// <param name="rate">Learning rate</param>
		static void learn(const backward_bias_data& bcd, bias_data& cd, float rate)
		{
			learn_check(bcd, cd);

			auto&& bbetav = bcd.bbetas;
			auto&& scalev = bcd.ld.scale;
			auto&& betav = cd.betas;

			for (auto c : cr)
			{
				betav[c] -= rate * scalev[c] * bbetav[~c];
			}
		}
	};

	/// <summary>
	/// The loss layer
	/// 
	/// The loss function is the sum of square differences between network output and ground truth.
	/// </summary>
	/// <typeparam name="CSP">Channel size policy</typeparam>
	template< typename CSP>
	struct loss_layer< CSP, permutation_policy>
		: loss_layer_base< CSP, permutation_policy>
	{
	private:
		using base_ = loss_layer_base< CSP, permutation_policy>;

		using typename base_::input_data;
		using typename base_::output_data;

		using typename base_::backward_input_data;

		using base_::cr;

		using base_::forward_check;
		using base_::backward_check;
	public:
		/// <summary>
		/// The forward-propagation function of the loss layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="gd">Ground-truth activations</param>
		/// <param name="outd">The loss</param>
		static void forward(const input_data& ind, const gold_data& gd, output_data& outd)
		{
			auto br = forward_check(ind, gd, outd);

			auto&& inv = ind.values;
			auto&& labelv = gd.labels;
			auto&& outv = outd.loss;

			for (auto b : br)
			{
				auto label = labelv[b];
				float loss = 0.0f;
				for (auto c : cr)
				{
					auto value = inv[c & b];
					auto expected = c.value() == label ? 1.0f : 0.0f;
					auto diff = value - expected;
					loss += diff * diff;
				}
				outv[b] = loss;
			}
		}

		/// <summary>
		/// The backward-propagation function of the loss layer
		/// </summary>
		/// <param name="ind">Input activations</param>
		/// <param name="gd">Ground-truth activations</param>
		/// <param name="bind">Loss derivatives wrt. the input activations</param>
		static void backward(const input_data& ind, const gold_data& gd, backward_input_data& bind)
		{
			auto br = backward_check(ind, gd, bind);

			auto&& inv = ind.values;
			auto&& labelv = gd.labels;
			auto&& binv = bind.bvalues;

			for (auto b : br)
			{
				auto label = labelv[b];
				for (auto c : cr)
				{
					auto value = inv[c & b];
					auto expected = c.value() == label ? 1.0f : 0.0f;
					auto diff = value - expected;
					binv[~c & ~b] = 2.0f * diff;
				}
			}
		}
	};
	/// @}
}

#endif
